{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CausalRL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8F7k8X-EFLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools as itt\n",
        "from collections import Counter\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "import torch\n",
        "from tabulate import tabulate\n",
        "import numpy as np \n",
        "import gym\n",
        "import pyro\n",
        "from gym.wrappers import TimeLimit\n",
        "from pyro.distributions import Categorical, Delta, Normal\n",
        "from pyro.infer import EmpiricalMarginal, Importance\n",
        "from statistics import mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQdFpeqhadMD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Causal Reinforcement learning\n",
        "Reinforcement learning (RL) is the study of how an agent (human, animal or machine) can learn to choose actions that maximize its future rewards. The system consists of an agent that interacts  with the environment via actions at discrete time steps and recieves a certain reward. \n",
        "![alt text](https://i.imgur.com/EQmZWDc.png?1)\n",
        "\n",
        "This problem is inherently causal in nature. We are interested in identifying the effect of taking an action and also the actions that give maximum rewards. This relates to the Bayesian Causal Inference where one can reason backwards to derive the causes based on the observation of effects. Applying this to the RL setting one can infer the best action based on the reward(Planning as inference). Can standard reinforcement learning derive this causal relationship between action and or is it merely an estimate of association.\n",
        "Today we have state of the art Reinforcement learning algorithms that surpass humans in environments on which they are trained, but they lack the ability to learn reusable human-like concepts. Even minor changes to the environment leave them confused. \n",
        "\n",
        "A DAG for sequential decision making in reinforcement learning : \n",
        "\n",
        "![image](https://i.imgur.com/jS7vzSJ.png?1)\n",
        "\n",
        "U here is a confounder that affects the action as well the outcomes(rewards) in the environment. Often these confounders are unobserved and in those situations the estimate of effect of an action on the outcome is biased. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx4g6l259_Rg",
        "colab_type": "text"
      },
      "source": [
        "# Bellman Equation\n",
        "The goal of an agent is to maximize the expected reward , which is defined as:\n",
        "\n",
        "$$Q(s, a) = E[R|s_0=s, a_0=a]$$\n",
        "\n",
        "The optimal action then for a given state s maximizes Q(s, a):\n",
        "\n",
        "$$a^*_s = argmax_a Q(s, a)$$\n",
        "\n",
        "Due to the markov property assumption we can express the optimal Q value as :\n",
        "$$Q^*(s, a) = R(s, a) + \\gamma \\sum_{s'}T(s'|s,a) max_{a'} Q^*(s', a')$$ \n",
        "\n",
        "## Actions as Interventions \n",
        "In the causal world we can define the expected reward as :\n",
        "\n",
        "$$Q(s, a) = E[R|s_0=s, do(a_0=a)]$$\n",
        "\n",
        "The optimal value function :\n",
        "$$Q^*(s, a) = R(s, a) + \\gamma \\sum_{s'}T(s'|s,a) max_{a'} [Q^*(s', a')| do(a = a')]$$ \n",
        "\n",
        "With actions as active interventions, we can now eliminate the confounding bias in the environment and arrive at a optimal policy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF0siNa6EMsv",
        "colab_type": "text"
      },
      "source": [
        "# Experiments on FrozenLake-V0 from OpenAI Gym\n",
        "Frozen Lake is a grid environment where some tiles are walkable and others lead the agent to fall into the water. Agent is rewarded for finding a walkable path to the goal. \n",
        "\n",
        "![alt text](https://i.imgur.com/g5gduIk.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ1npZMtKhdZ",
        "colab_type": "text"
      },
      "source": [
        "A binary confounder was added to the environment. \n",
        "This confounder determines the action and reward distribution at each timestep. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSgXxNvHEIg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class FrozenLakeWrapper(gym.Wrapper):\n",
        "    \"\"\"FrozenLakeWrapper\n",
        "\n",
        "    Wrapper around a FrozenLake environment; implements a time limit and reward\n",
        "    shaping.\n",
        "    Also includes a confounder\n",
        "    :param env:  OpenAI Gym environment\n",
        "    :param max_episode_steps:  problem horizon\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, *, max_episode_steps):\n",
        "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
        "        super().__init__(env)\n",
        "        self.actions = ['left', 'down', 'right', 'up']\n",
        "        self.u = self.get_confounder()\n",
        "        self.episodes = 0\n",
        "\n",
        "    # generate a binary confounder \n",
        "    def get_confounder(self):\n",
        "        return pyro.sample('u', Categorical(torch.tensor([1., 1.])))\n",
        "\n",
        "    # get the reward based on the action and confounder\n",
        "    def utility(self, action):\n",
        "        r_vals = [0.2, 0.25]\n",
        "        r_probs = [[[0.8, 0.2], [0.75, 0.25], [0.7, 0.3], [0.85, 0.15]], \n",
        "                [[0.6, 0.4], [0.65, 0.35], [0.7, 0.3], [0.8, 0.2]]]\n",
        "        rc = pyro.sample(f'rc_{self.episodes}', Categorical(torch.tensor(r_probs[self.u][action])))\n",
        "        self.episodes = self.episodes + 1\n",
        "        return r_vals[rc.item()]\n",
        "    \n",
        "    # confounded action distribution\n",
        "    def action_dist_confounded(self):\n",
        "        action_probs = [[0.45, 0.45, 0.2, 0.2],[0.55, 0.55, 0.8, 0.8]]\n",
        "        return Categorical(torch.tensor(action_probs[self.u]))\n",
        "\n",
        "\n",
        "    # take a step in the environment\n",
        "    def step(self, action):\n",
        "\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "\n",
        "        if info.get('TimeLimit.truncated', False):\n",
        "            # negative reward if time limit is reached\n",
        "            reward = -1.0\n",
        "        elif done and reward == 0.0:\n",
        "            # negative reward for falling in a hole\n",
        "            reward = -1.0\n",
        "\n",
        "        # reward has the original value\n",
        "        # modify it to reflect the effect of confounding\n",
        "        if reward == 0.:\n",
        "            reward = reward + self.utility(action)\n",
        "\n",
        "        return observation, reward , done, info\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMJUxvaDPLjw",
        "colab_type": "text"
      },
      "source": [
        "Causal model of the environment, confounded action and reward distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1juC_ZnPPwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ACTIONS = [0, 1, 2, 3]\n",
        "# causal model of the environment\n",
        "def action_distribution(u=0):\n",
        "    action_probs = torch.tensor([[0.25, 0.35, 0.2, 0.2],  # u = 0\n",
        "                                 [0.75, 0.65, 0.8, 0.8]]) # u = 1\n",
        "    return Categorical(action_probs[u])\n",
        "\n",
        "def reward_distribution(state, action, u):\n",
        "    # original reward distribution\n",
        "    if state == 15.:\n",
        "      reward = 1.\n",
        "    elif state in [5, 7, 11, 12]:\n",
        "      reward = -1.\n",
        "    else:\n",
        "      reward = 0. \n",
        "    # confounded distribution\n",
        "    r_vals = [0.2, 0.4] # incremental reward value\n",
        "    r_probs = [[[0.8, 0.2], [0.75, 0.25], [0.7, 0.3], [0.85, 0.15]], # u=0, action = a\n",
        "                [[0.6, 0.4], [0.65, 0.35], [0.7, 0.3], [0.8, 0.2]]] # u = 1, action = a \n",
        "    rc = Categorical(torch.tensor(r_probs[int(u)][int(action)])).sample()\n",
        "    if reward == 0. :\n",
        "       reward += r_vals[int(rc)]\n",
        "    # sample reward from Dirac\n",
        "    return pyro.sample('reward', Normal(torch.tensor(reward), torch.tensor(0.00001)))\n",
        "\n",
        "\n",
        "def model(env):\n",
        "    \"\"\"Model of the environment\"\"\"\n",
        "    u = pyro.sample('u', Categorical(torch.tensor([1., 1.])))\n",
        "    action = pyro.sample('action', action_distribution(u))\n",
        "    observation, _, done, info = env.step(int(action))\n",
        "    reward = reward_distribution(observation, action, u)\n",
        "    return env, observation, reward, done, info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5lJyRHREOOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def argmax(iterable, func):\n",
        "    \"\"\"Get the argmax of an iterable\"\"\"\n",
        "    return max(iterable, key=func)\n",
        "\n",
        "def manhattan_distance(s, gridsize=4):\n",
        "    \"\"\" Get the manhattan distance from goal\"\"\"\n",
        "    # 2 dimensional vectors, s/4 gives row index , s%4 gives column index for a 4 * 4 grid\n",
        "    p = np.array([int(s / gridsize), (s % gridsize)])\n",
        "    q = np.array([gridsize, gridsize])\n",
        "    return sum(abs(p - q))\n",
        "\n",
        "def expected_reward(Q_function, action, env, u, i=0):\n",
        "    def get_posterior_mean(posterior, n_samples=30):\n",
        "        \"\"\"\n",
        "        Calculate posterior mean\n",
        "        \"\"\"\n",
        "        # Sample\n",
        "        marginal_dist = EmpiricalMarginal(posterior).sample((n_samples, 1)).float()\n",
        "        # assumed to be all the same\n",
        "        return torch.mean(marginal_dist)\n",
        "    # The use of the param store is an optimization\n",
        "    param_name = 'posterior_reward_state{}_{}'.format(env.s, i)\n",
        "    if param_name in list(pyro.get_param_store().keys()):\n",
        "        posterior_mean = pyro.get_param_store().get_param(param_name)\n",
        "        return posterior_mean\n",
        "    else:\n",
        "        # this gets slower as we increase num_samples\n",
        "        inference = Importance(Q_function, num_samples=30)\n",
        "        posterior = inference.run(action, env, u)\n",
        "        posterior_mean = get_posterior_mean(posterior, 30)\n",
        "        pyro.param(param_name, posterior_mean)\n",
        "        return posterior_mean\n",
        "\n",
        "\n",
        "def imagine_next_step(env, action):\n",
        "    \"\"\"Agent imagines next time step\"\"\"\n",
        "    sim_env = deepcopy(env)\n",
        "    state = sim_env.s\n",
        "    # when I intervene I should also make sure u does not change over time in this environment\n",
        "    int_model = pyro.do(model, {'action': action})\n",
        "    sim_env, _, _, _, _ = int_model(sim_env)\n",
        "    # sanity check\n",
        "    assert sim_env.lastaction == action\n",
        "    return sim_env\n",
        "\n",
        "\n",
        "def Q(action, env, u=0):\n",
        "    \"\"\"Q function variant of Bellman equation.\"\"\"\n",
        "    utility = reward_distribution(env.s, action, u)\n",
        "    if utility not in [1., -1]:\n",
        "        env_step = imagine_next_step(env, action)\n",
        "    # check if the action got us closer to the goal. if yes only then recurse\n",
        "        if (env.s != env_step.s) and (manhattan_distance(env.s) >= manhattan_distance(env_step.s)) :\n",
        "            # Calculate expected rewards for each action but\n",
        "            # exclude backtracking actions.\n",
        "            expected_rewards = [Q(act, env_step, u)\n",
        "                for j, act in enumerate(ACTIONS)\n",
        "                if ACTIONS[abs(j - 2)] != action\n",
        "            ]\n",
        "            # Choose reward from optimal action\n",
        "            utility = utility + max(expected_rewards)\n",
        "    return utility\n",
        "\n",
        "\n",
        "\n",
        "def policy(real_env, u,i=0):\n",
        "    # Choose optimal action\n",
        "    action = argmax(ACTIONS, partial(Q, env=real_env, u=u ))\n",
        "    print(action)\n",
        "    return action\n",
        "\n",
        "\n",
        "def main(fl_env):\n",
        "    \"\"\"A walk through the environment by choosing optimal actions\"\"\"\n",
        "    u = fl_env.u\n",
        "    for t in range(100):\n",
        "        pyro.clear_param_store()\n",
        "        action = policy(fl_env, u) \n",
        "        int_model = pyro.do(model,\n",
        "                            {'action': torch.tensor(action)})\n",
        "        fl_env, observation, reward, done, info = int_model(fl_env)\n",
        "        fl_env.render()\n",
        "        if done:\n",
        "            print(\"Episode finished after {} timesteps\".format(t + 1))\n",
        "            break\n",
        "    fl_env.close()\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXkg2IvvEWe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
        "fl_env = FrozenLakeWrapper(env, max_episode_steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJdZgwc_elF",
        "colab_type": "text"
      },
      "source": [
        "The agent is able to finally the reach the goal but it may not have made the optimal choice at each step. If it did, it should reach the goal in 6 steps "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGJhqou5lJOM",
        "colab_type": "code",
        "outputId": "3cf8f09c-025b-402e-d1ee-4f138bf06ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# render the results\n",
        "fl_env.reset()\n",
        "main(fl_env)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "2\n",
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "2\n",
            "  (Right)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "3\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0\n",
            "  (Left)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "2\n",
            "  (Right)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0\n",
            "  (Left)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "1\n",
            "  (Down)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Episode finished after 12 timesteps\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}